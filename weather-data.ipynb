{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation of Weather Data\n",
    "The following notebook demonstrates how weather data was scraped from airdensity online. A site that provides free to use weather data for corresponding race tracks. In this instance, Formula 1 circuits.\n",
    "\n",
    "The site can be found [here.]('https://airdensityonline.com/tracks/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Helper Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12 Hour to 24 Hour Time Fuction\n",
    "Converts the scraped time format into more readble 24-hour time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert 12-hour time format to 24-hour\n",
    "def convert24(raw): \n",
    "      \n",
    "    # Checking if last two elements of time \n",
    "    # is AM and first two elements are 12 \n",
    "    if raw[-2:] == \"am\" and raw[:2] == \"12\": \n",
    "        return \"00\" + raw[2:-3] + \":00\"\n",
    "    \n",
    "    # Check for e.g. 10:00 am, 11:00 am\n",
    "    elif raw[-2:] == \"am\" and (raw[:2] == \"11\" or raw[:2] == \"10\"):\n",
    "        return raw[:-3] + \":00\"\n",
    "        \n",
    "    # remove the AM     \n",
    "    elif raw[-2:] == \"am\": \n",
    "        return \"0\" + raw[:-3] + \":00\"\n",
    "      \n",
    "    # Checking if last two elements of time \n",
    "    # is PM and first two elements are 12    \n",
    "    elif raw[-2:] == \"pm\" and raw[:2] == \"12\": \n",
    "        return raw[:5] + \":00\"\n",
    "          \n",
    "    elif raw[2] == \":\": \n",
    "        # add 12 to hours and remove PM \n",
    "        return str(int(raw[:2]) + 12) + raw[2:5] + \":00\"\n",
    "    \n",
    "    else: \n",
    "        # add 12 to hours and remove PM \n",
    "        return str(int(raw[:1]) + 12) + \":\" + raw[2:4] + \":00\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fahrenheit to Celsius Function\n",
    "Changes temp from F to C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'null'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f_to_c(temp):\n",
    "    \n",
    "    if temp != \"inf\":\n",
    "        return round((int(temp) - 32) * 5/9)\n",
    "    else:\n",
    "        return 'null'\n",
    "\n",
    "f_to_c(\"inf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for negative or nan value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checks specifically the grain for negative or nan value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'null'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_neg_nan(string):\n",
    "    \n",
    "    if string == \"nan\":\n",
    "        return 'null'\n",
    "    elif float(string) < 0:\n",
    "        return 'null'\n",
    "    else:\n",
    "        return string\n",
    "    \n",
    "check_neg_nan(\"nan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Website Weather Data\n",
    "Here we scrape the data from the weather website, modifying certain attributes for easier processing.\n",
    "You are able to specify the url of the site, and path you want to save the file for.\n",
    "\n",
    "**Note: This only works for links such as https://airdensityonline.com/track-history/Melbourne_Grand_Prix_Circuit/2020-03-21/**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_weather(url, path):\n",
    "    # Prepare response raw data\n",
    "    response = requests.get(url)\n",
    "    soup = bs(response.content, 'html.parser')\n",
    "    rows = soup.find(class_ = 'forecastdata').find_all('ul', recursive=False)[1:]\n",
    "\n",
    "    # Create empty csv for writing\n",
    "    file = open('data/' + path, 'w', newline='')\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['time', 'temp', 'dew_point', 'humidity', 'grains', 'wind_speed', 'wind_direction'])\n",
    "\n",
    "    # Iterate over rows and scrape data\n",
    "    for row in rows:\n",
    "        time = row.find_all('li')[0].text.strip()[7:]\n",
    "        temp = row.find_all('li')[1].text.strip()[:-6]\n",
    "        dew_point = row.find_all('li')[4].text.strip()[:-6]\n",
    "        humidity = row.find_all('li')[2].text.strip()[:-1]\n",
    "        grains = row.find_all('li')[5].text.strip()\n",
    "        wind = row.find_all('li')[6].text.strip()[:-4]\n",
    "        wind_speed = wind.split()[1]\n",
    "        wind_direction = wind.split()[0]\n",
    "\n",
    "        # Convert particular data\n",
    "        time = convert24(time)\n",
    "        temp = f_to_c(temp)\n",
    "        dew_point = f_to_c(dew_point)\n",
    "        grains = check_neg_nan(grains)\n",
    "        \n",
    "        writer.writerow([time, temp, dew_point, humidity, grains, wind_speed, wind_direction])\n",
    "    \n",
    "    file.close()\n",
    "    return \"File Generated. Size: \" + str(os.stat('data/' + path).st_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'File Generated. Size: 766'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_weather('https://airdensityonline.com/track-history/Autodromo_de_Interlagos_Portao/2018-11-11/', 'weather/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "british_gp_url = ['2016', '2017', '2018', '2019']\n",
    "british_gp_dates = \"https://airdensityonline.com/track-history/Autodromo_de_Interlagos_Portao/\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
